{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Копия блокнота \"ДЗ39 Tensorflow 2 Власов АЛ\"",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASv-k4FEP_8S"
      },
      "source": [
        "# **LIGHT** \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iczH7mSwP1W5"
      },
      "source": [
        "import tensorflow as tf #Импортируем tensorflow\n",
        "import datetime, os #Для подсчета времени и работы с файловой системой\n",
        "import numpy as np #Для работы с матрицами \n",
        "from tensorflow.keras import utils #Для работы с категориальными данными \n",
        "import sys #Для специльного вывода\n",
        "import matplotlib.pyplot as plt #Для виузализации \n",
        "import random #Для генерации случайных чисел "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RgclOSLQqZJ"
      },
      "source": [
        "**Универсальная функция для формирования датасета** https://www.youtube.com/watch?v=ljMOTWrszxQ&t=326s\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wipFg2VzfyBa"
      },
      "source": [
        "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0PXhAoogA4U"
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F738EtHF52GV"
      },
      "source": [
        "y_train[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-TLgpbIfw_A"
      },
      "source": [
        "#Функция формирования датсета mnist \n",
        "#flatten - вытягивание картинки из формы 28, 28 в 784\n",
        "def data_cifar10(flatten = True):\n",
        "  #Скачивааем датасет \n",
        "  (X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "    \n",
        "  if(flatten):#Для полносвязной сети  \n",
        "    X_train = X_train.reshape(-1, 32*32*3)\n",
        "    X_test = X_test.reshape(-1, 32*32*3)\n",
        "  else: #Для сверточной сети \n",
        "    X_train = X_train.reshape(-1, 32, 32, 3)\n",
        "    X_test = X_test.reshape(-1, 32, 32, 3)\n",
        "  #Первеодим во float\n",
        "  X_train = X_train.astype('float32')\n",
        "  X_test = X_test.astype('float32')\n",
        "  #Переводим все значения в диапазон от 0 до 1\n",
        "  X_train /= 255\n",
        "  X_test /= 255\n",
        "\n",
        "  print('Форма x_train:', X_train.shape)\n",
        "  print(X_train.shape[0], 'обучающих примеров')\n",
        "  print(X_test.shape[0], 'проверочных примеров')\n",
        "  #Переводим в OneHot\n",
        "  y_train = utils.to_categorical(y_train, 10).astype(np.float32)\n",
        "  y_test = utils.to_categorical(y_test, 10).astype(np.float32)\n",
        "  #Возвращаем датасет\n",
        "  return X_train, y_train, X_test, y_test #Возвращаем датасет "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KR0hcUZImxgD"
      },
      "source": [
        "**Загрузка данных** https://www.youtube.com/watch?v=ljMOTWrszxQ&t=427s\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_RF7RpEQ5sx"
      },
      "source": [
        "x_train, y_train, x_test, y_test = data_cifar10() #Загружаем датасет "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fs02pgecQ7rW"
      },
      "source": [
        "**Параметры обучения и оптимизации** https://www.youtube.com/watch?v=ljMOTWrszxQ&t=444s\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r36_FDdZE7T_"
      },
      "source": [
        "classes = {\n",
        "    0 : 'airplane',\n",
        "    1 : 'automobile',\n",
        "    2 : 'bird',\n",
        "    3 : 'cat',\n",
        "    4 : 'deer',\n",
        "    5 : 'dog',\n",
        "    6 : 'frog',\n",
        "    7 : 'horse',\n",
        "    8 : 'ship',\n",
        "    9 : 'truck'\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15RObI_01XK-"
      },
      "source": [
        "## **Сверточные нейронные сети** \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idT31JTgfswW"
      },
      "source": [
        "padding = \"SAME\"  \n",
        "num_output_classes = 10\n",
        "batchSize = 256\n",
        "epochs = 100\n",
        "learningRate = 0.001 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "re2LkUyRfuMH"
      },
      "source": [
        "leaky_relu_alpha = 0.2\n",
        "dropout_rate = 0.5 \n",
        "\n",
        "def conv2d( inputs , filters , stride_size ): #Слой для создания сверточного слоя \n",
        "    out = tf.nn.conv2d( inputs , filters , strides=[ 1 , stride_size , stride_size , 1 ] , padding=padding ) \n",
        "    return tf.nn.leaky_relu(out , alpha=leaky_relu_alpha ) \n",
        "\n",
        "def maxpool( inputs , pool_size , stride_size ): #Слой для применения maxpooling\n",
        "    return tf.nn.max_pool2d(inputs , ksize=[ 1 , pool_size , pool_size , 1 ] , padding='VALID' , strides=[ 1 , stride_size , stride_size , 1 ] )\n",
        "\n",
        "def dense(inputs , weights): #Слой для создания полносвязного слоя \n",
        "    x = tf.nn.leaky_relu(inputs @ weights, alpha=leaky_relu_alpha )\n",
        "    return tf.nn.dropout( x , rate=dropout_rate )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZAT7FybfvUh"
      },
      "source": [
        "output_classes = 10 #Определяем число классов  https://www.youtube.com/watch?v=ljMOTWrszxQ&t=3216s\n",
        "initializer = tf.initializers.glorot_uniform() #Инициализатор переменных по форме\n",
        "def get_weight(  shape, name ): #Функция для получения весов\n",
        "    return tf.Variable(initializer(shape) , name=name , trainable=True , dtype=tf.float32 )\n",
        "\n",
        "#Формы слоев \n",
        "shapes = [\n",
        "    [ 1 , 1 , 3 , 16 ] , \n",
        "    [ 2 , 2 , 16 , 16 ] , \n",
        "    [ 2 , 2 , 16, 32 ] , \n",
        "    [ 2 , 2 , 32 , 32 ] ,\n",
        "    [ 2 , 2 , 32 , 64 ] , \n",
        "    [ 2 , 2 , 64 , 64 ] ,\n",
        "    [ 1024 , 32 ] , \n",
        "    [ 32 , output_classes ] ,\n",
        "]\n",
        "\n",
        "#Создание весов https://www.youtube.com/watch?v=ljMOTWrszxQ&t=3799s\n",
        "weights = []\n",
        "for i in range(len(shapes)):\n",
        "    weights.append( get_weight(shapes[i] , 'weight{}'.format( i )))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1GzBWyxfwU8"
      },
      "source": [
        "#Модель https://www.youtube.com/watch?v=ljMOTWrszxQ&t=3846s \n",
        "def model( x ) :\n",
        "    x = tf.cast( x , dtype=tf.float32 )\n",
        "    c1 = conv2d( x , weights[0] , stride_size=1 ) \n",
        "    c1 = conv2d( c1 , weights[1] , stride_size=1 )\n",
        "    p1 = maxpool( c1 , pool_size=2 , stride_size=2 )\n",
        "    \n",
        "    c2 = conv2d( p1 , weights[2] , stride_size=1 )\n",
        "    c2 = conv2d( c2 , weights[3] , stride_size=1 ) \n",
        "    p2 = maxpool( c2 , pool_size=2 , stride_size=2 )\n",
        "    \n",
        "    c3 = conv2d( p2 , weights[4] , stride_size=1 ) \n",
        "    c3 = conv2d( c3 , weights[5] , stride_size=1 ) \n",
        "    p3 = maxpool( c3 , pool_size=2 , stride_size=2)\n",
        "    flatten = tf.reshape( p3 , shape=( tf.shape( p3 )[0] , -1 ))\n",
        "\n",
        "    d1 = dense( flatten , weights[6])\n",
        "    logits = tf.matmul( d1 , weights[7] )\n",
        "\n",
        "    return tf.nn.softmax(logits)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_aMoJFj7fxhP"
      },
      "source": [
        "def loss( pred , target ): #Функция подсчета ошибки  https://www.youtube.com/watch?v=ljMOTWrszxQ&t=4107s\n",
        "    return tf.losses.categorical_crossentropy( target , pred )\n",
        "\n",
        "optimizer = tf.optimizers.Adam(learningRate)\n",
        "\n",
        "def train( model, inputs , outputs ):\n",
        "    m = tf.keras.metrics.Accuracy() #Задаем метрику\n",
        "    with tf.GradientTape() as tape:\n",
        "        current_loss = loss( model( inputs ), outputs)\n",
        "    # Градиентный спуск. Инициализируем через learning rate\n",
        "    # Функция реализует градиентный спуск и обратное распространение ошибки.\n",
        "    grads = tape.gradient( current_loss , weights )\n",
        "    #Применение градиентного спуска\n",
        "    optimizer.apply_gradients( zip( grads , weights ) )\n",
        "    #Подсчет точности сети\n",
        "    m.update_state(np.argmax(outputs, axis=1), np.argmax(model(inputs), axis=1))\n",
        "    return tf.reduce_mean(current_loss) , m.result().numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4VCZSR1fypC"
      },
      "source": [
        "path = '/tmp/mylogs/eager/' #Указываем путь для сохранения даных для Tensorboard https://www.youtube.com/watch?v=ljMOTWrszxQ&t=4166s\n",
        "\n",
        "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") #Получаем информацию о текущем времени для добавления данной информации к имени  \n",
        "loss_log_dir = path + current_time + '/data' #Создаем папку с именем из текущего времени\n",
        "loss_summary_writer = tf.summary.create_file_writer(loss_log_dir) #Создаем средство записи файла резюме для данного каталога\n",
        "\n",
        "amount_bathces = int(len(x_train) / batchSize) #Считаем число батчей для каждой эпохи. Понадобится для вывода\n",
        "\n",
        "with loss_summary_writer.as_default(): #Используя созданное средство для записи файла в резюме \n",
        "  \n",
        "  for epoch in range(1, epochs + 1):\n",
        "    learningEpochStartTime = datetime.datetime.now() #Запоминаем время начала эпохи \n",
        "    print('Эпоха', epoch, '/', epochs) #Пишем текущую эпоху и общее число эпох\n",
        "    avg_loss = 0 #Задаем среднюю ошибку \n",
        "    for batch in range(0, len(x_train), batchSize):\n",
        "      current_loss, accuracy = train( model , x_train[batch:batch + batchSize] , y_train[batch:batch + batchSize] )\n",
        "      avg_loss += current_loss\n",
        "\n",
        "      params = {'Длительность обучения на эпохе: ': datetime.datetime.now() - learningEpochStartTime, #Считаем время обучения на данной эпохе и добавляем в словарь\n",
        "                'loss: ': current_loss.numpy(), #Переводим ошибку в Numpy и добавляем в словарь\n",
        "                'accuracy: ': accuracy} #Добавляем точность в словарь\n",
        "      if(batch >= len(x_train) - batchSize): #На последнем батче\n",
        "        params['loss: '] = (avg_loss / amount_bathces).numpy() #Выводим среднюю ошибку на всей эпохе\n",
        "      current_batch = int(batch / batchSize) + 1 #Считаем номер текущего батча\n",
        "      print_log(current_batch, amount_bathces, params)\n",
        "    tf.summary.scalar(\"avg_loss\", avg_loss, step=epoch) #Сохраняем данные для Tensorboard\n",
        "    tf.summary.scalar(\"accuracy\", accuracy, step=epoch) #Сохраняем данные для Tensorboard\n",
        "    loss_summary_writer.flush() #Очищаем буфер вывода \n",
        "    print() #Вручную переносим каретку на следующую строку, чтобы не стирать финальные значения сети на эпохе\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5rej7I3fzrq"
      },
      "source": [
        "#Выводим для примера картинки по каждому классу https://www.youtube.com/watch?v=ljMOTWrszxQ&t=4402s\n",
        "print('Сеть распознала цифры: ')\n",
        "fig, axs = plt.subplots(1, 10, figsize=(25, 3)) #Создаем полотно из 10 графиков\n",
        "for i in range(10): #Проходим по классам от 0 до 9\n",
        "  label_indexes = np.where(np.argmax(y_test, axis = 1)==i)[0] #Получаем список из индексов положений класса i в y_test\n",
        "  index = random.choice(label_indexes) #Случайным образом выбираем из списка индекс\n",
        "  img = x_test[index] #Выбираем из x_train нужное изображение\n",
        "  axs[i].imshow(img.reshape(32, 32, 3)) #Отображаем изображение i-ым графиков\n",
        "  print(classes[np.argmax(model([img]))], end=' ')\n",
        "plt.show() #Показываем изображения"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNJYHwIXNKAS"
      },
      "source": [
        "## Визуализация\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJ8jBqvuo2ns"
      },
      "source": [
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTRUz78vozqN"
      },
      "source": [
        "%tensorboard --logdir '/tmp/mylogs/eager/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOHgmTEbm2tv"
      },
      "source": [
        "**Тренируемые параметры** https://www.youtube.com/watch?v=ljMOTWrszxQ&t=957s\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bx2TG5tjTwyp"
      },
      "source": [
        "trainableParams = [] #Лист тренируемых параметров\n",
        "# Объявляем веса, W1\n",
        "trainableParams.append(tf.Variable(tf.random.normal([3072, 300], stddev=0.03), name='W1')) #300 размер скрытого слоя, инциалзириуем значения\n",
        "\n",
        "# Используя нормальное распределение со средним ноль и статистическим отклонением 0.03, определяем bias(Аналогичная есть у numpy)\n",
        "trainableParams.append(tf.Variable(tf.random.normal([300], stddev=0.03), name='b1'))\n",
        "\n",
        "# То же делаем для весов и bias(отклонения) от скрытого к выходному\n",
        "trainableParams.append(tf.Variable(tf.random.normal([300, 10], stddev=0.03), name='W2'))\n",
        "trainableParams.append(tf.Variable(tf.random.normal([10]), name='b2'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hERkriMDRBLz"
      },
      "source": [
        "#Функция подсчета ошибки https://www.youtube.com/watch?v=ljMOTWrszxQ&t=1151s\n",
        "def loss(pred , target):\n",
        "    return tf.losses.categorical_crossentropy(target , pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJ-VrWV_RB1v"
      },
      "source": [
        "#Полносвязный слой с несколькими функциями активации https://www.youtube.com/watch?v=ljMOTWrszxQ&t=1291s\n",
        "def base(x , params):\n",
        "    W1 = params[0]\n",
        "    b1 = params[1]\n",
        "    W2 = params[2]\n",
        "    b2 = params[3]\n",
        "\n",
        "    #a @ b - аналог tf.matmul(a, b)\n",
        "    hiddenOut = tf.nn.relu(x@W1+b1) #Умножаем вход на веса W1, прибавляем b1 и применяем функцию активации relu\n",
        "    y = tf.nn.softmax(hiddenOut@W2+b2) #Умножаем выход скрытого слоя на веса W2, прибавляем b2 и применяем функцию активации softmax\n",
        "    return y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2LeDDs_RyD9"
      },
      "source": [
        "#Модель  https://www.youtube.com/watch?v=ljMOTWrszxQ&t=1541s\n",
        "def model(x):\n",
        "    y = base(x, trainableParams)\n",
        "    return y \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JOUtn607XGO"
      },
      "source": [
        "x_train[1].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NeeRAPT1Rtxy"
      },
      "source": [
        "#Посмотрим, какой выход у модели, если на вход подать случайную картинку \n",
        "model(x_train[[1]]) #Смотрим вывод сети для первого изображения из MNIST https://www.youtube.com/watch?v=ljMOTWrszxQ&t=1559s"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZimjXOi7Qu8"
      },
      "source": [
        "y_train[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3ENT9j5WDTW"
      },
      "source": [
        "#Функция для информативного вывода обучения https://www.youtube.com/watch?v=ljMOTWrszxQ&t=1665s\n",
        "#На вход получает:\n",
        "# current - номер текущего батча\n",
        "# amount - число всех батчей \n",
        "# params - словарь дополнительных параметров для вывода \n",
        "\n",
        "def print_log(current, amount, params):\n",
        "  #Мы будем выводить прогрессбар, подобный прогрессбару при обучении нейронных сетей на Keras\n",
        "  #Формат прогрессбара 23/120 [=====>------------------------]\n",
        "  \n",
        "  bar_len = 30 #Длина бара \n",
        "  percent = int(current * bar_len / amount) #Процент выполненной работы\n",
        "  progressbar = ''\n",
        "\n",
        "  for i in range(bar_len): #Проходим по всем элементам прогрессбара и добавляем символы в соответсвии с прогресом \n",
        "    if(i < percent):\n",
        "      progressbar += '='\n",
        "    elif(i == percent):\n",
        "      progressbar += '>'\n",
        "    else:\n",
        "      progressbar += '-'\n",
        "\n",
        "  #Добавялем в финальное сообщение символ переноса коретки консоли на начальную строку, добавляпем информацию о номере батча\n",
        "  #количестве всех батчей, прогрессбар\n",
        "  #Символ переноса коретки \\r добавляется для того, чтобы каждый новый батч перезаписывать вывод. Таким образом вывод не будет засоряться повторяющейся информацией\n",
        "  message = \"\\r\" + str(current) + '/' + str(amount) + ' [' + progressbar + ']  ' \n",
        "  #Добавляем дополнительные параметры в вывод\n",
        "  for key in params:\n",
        "    message += key + str(params[key]) + '. '\n",
        "  \n",
        "  print(message, end='')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3FY0QRpTa1r"
      },
      "source": [
        "optimizer = tf.keras.optimizers.SGD(learning_rate=learningRate) #Задаем оптимизатор https://www.youtube.com/watch?v=ljMOTWrszxQ&t=1860s\n",
        "m = tf.keras.metrics.Accuracy() #Задаем метрику\n",
        "\n",
        "def train(model, inputs, outputs): #Функция тренировки сети \n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "      current_loss = tf.reduce_mean(loss(model(inputs), outputs)) #Считаем ошибку\n",
        "\n",
        "      # Градиентный спуск. Инициализируем через learning rate\n",
        "      # Функция реализует градиентный спуск и обратное распространение ошибки.\n",
        "      grads = tape.gradient(current_loss , trainableParams)\n",
        "      #Применение градиентного спуска\n",
        "      optimizer.apply_gradients(zip(grads , trainableParams))\n",
        "      #Подсчет точности сети \n",
        "      m.update_state(np.argmax(outputs, axis=1), np.argmax(model(inputs), axis=1))\n",
        "    return current_loss, m.result().numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "flwDnYKsZL4v"
      },
      "source": [
        "!rm -R /tmp/mylogs/eager #Очищаем папку с записанной информацией обучения (если код запускается не в первый раз)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDKGa1IyXUwp"
      },
      "source": [
        "path = '/tmp/mylogs/eager/' #Указываем путь для сохранения даных для Tensorboard https://www.youtube.com/watch?v=ljMOTWrszxQ&t=2237s\n",
        "\n",
        "\n",
        "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") #Получаем информацию о текущем времени для добавления данной информации к имени  \n",
        "loss_log_dir = path + current_time + '/data' #Создаем папку с именем из текущего времени\n",
        "loss_summary_writer = tf.summary.create_file_writer(loss_log_dir) #Создаем средство записи файла резюме для данного каталога\n",
        "\n",
        "amount_bathces = int(len(x_train) / batchSize) #Считаем число батчей для каждой эпохи. Понадобится для вывода\n",
        "\n",
        "with loss_summary_writer.as_default(): #Используя созданное средство для записи файла в резюме \n",
        " \n",
        "  for epoch in range(1, epochs + 1): #Проходим по каждой эпохе\n",
        "    learningEpochStartTime = datetime.datetime.now() #Запоминаем время начала эпохи \n",
        "    print('Эпоха', epoch , '/', epochs) #Пишем текущую эпоху и общее число эпох\n",
        "    avg_loss = 0 #Задаем среднюю ошибку\n",
        "\n",
        "    for batch in range(0, len(x_train), batchSize): #Проходим по x_train с шагом batchSize\n",
        "      current_loss, accuracy = train(model, x_train[batch: batch + batchSize], y_train[batch: batch + batchSize]) #Тренируем сеть и получаем значение ошибки\n",
        "      avg_loss += current_loss #Считаем среднюю ошибку\n",
        "\n",
        "      #Задаем параметры, которые будем выводить \n",
        "      params = {'Время обучения на эпохе: ': datetime.datetime.now() - learningEpochStartTime, #Считаем время обучения на данной эпохе и добавляем в словарь\n",
        "                'loss: ': round(current_loss.numpy(), 4), #Переводим ошибку в Numpy и добавляем в словарь\n",
        "                'accuracy: ': round(accuracy, 4)} #Добавляем точность в словарь\n",
        "      if(batch >= len(x_train) - batchSize): #На последнем батче\n",
        "        params['loss: '] = round((avg_loss / amount_bathces).numpy(), 4) #Выводим среднюю ошибку на всей эпохе\n",
        "\n",
        "      current_batch = int(batch / batchSize) + 1 #Считаем номер текущего батча\n",
        "      print_log(current_batch, amount_bathces, params) #Выводим всю нужную информацию \n",
        "    \n",
        "    tf.summary.scalar(\"avg_loss\", avg_loss, step=epoch) #Сохраняем данные для Tensorboard\n",
        "    tf.summary.scalar(\"accuracy\", accuracy, step=epoch) #Сохраняем данные для Tensorboard\n",
        "    loss_summary_writer.flush() #Очищаем буфер вывода \n",
        "    print() #Вручную переносим каретку на следующую строку, чтобы не стирать финальные значения сети на эпохе"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXJWUxsrd9LF"
      },
      "source": [
        "#Выводим для примера картинки по каждому классу https://www.youtube.com/watch?v=ljMOTWrszxQ&t=2481s\n",
        "print('Сеть распознала цифры: ')\n",
        "fig, axs = plt.subplots(1, 10, figsize=(25, 3)) #Создаем полотно из 10 графиков\n",
        "for i in range(10): #Проходим по классам от 0 до 9\n",
        "  label_indexes = np.where(np.argmax(y_test, axis = 1)==i)[0] #Получаем список из индексов положений класса i в y_test\n",
        "  index = random.choice(label_indexes) #Случайным образом выбираем из списка индекс\n",
        "  img = x_test[index] #Выбираем из x_train нужное изображение\n",
        "  axs[i].imshow(img.reshape(32, 32, 3)) #Отображаем изображение i-ым графиков\n",
        "  print(np.argmax(model([img])), end=' ')\n",
        "plt.show() #Показываем изображения"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQqa8snjeA7p"
      },
      "source": [
        "#PRO Оценка стоимости квартир\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-_Fvhdne4h6"
      },
      "source": [
        "Парсинг данных и архитектуру сети взял с занятия по регрессии"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvq1nzm5NdSH"
      },
      "source": [
        "import pandas as pd\n",
        "from google.colab import files\n",
        "from tensorflow.keras import utils\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from IPython.display import clear_output\n",
        "\n",
        "from datetime import datetime"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkSl5nb_OSyz"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "106DlPFbeKv9"
      },
      "source": [
        "df = pd.read_csv('/content/drive/My Drive/Colab Notebooks/19 Регрессия/moscow (1).csv', sep=\";\") \n",
        "df = df.iloc[::2,:] #Выбираем нечётные строки, в чётных строках в исходном фрейме пустые строки для комментариев\n",
        "data = df.values #Вытаскиваем данные в numpy array"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GV510wy8ePuS"
      },
      "source": [
        "df.head(20) #Показываем пример данных (первые 6 колонок)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPA51b_SkzxN"
      },
      "source": [
        "print(df.columns) #Показываем имена колонок данных\n",
        "#Мы будем использовать\n",
        "\n",
        "#В xTrain - база числовых значени\n",
        "#Комнат\n",
        "#Метро / ЖД станции\n",
        "#От станции\n",
        "#Дом\n",
        "#Балкон\n",
        "#Санузел\n",
        "#Площадь\n",
        "\n",
        "#В xTrainC - база текстов о квартирах\n",
        "#Примечание\n",
        "\n",
        "#В yTrain\n",
        "#Цена, руб.\n",
        "\n",
        "#Остальные колонки игнорируем"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVeq3Wdnk3Px"
      },
      "source": [
        "##Функции парсинга\n",
        "\n",
        "*Разбор данного раздела:* https://youtu.be/PCMkFDuxvCI?t=5834\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YPk-2eFk_O0"
      },
      "source": [
        "#Во всех функция дальше\n",
        "#d - описание квартиры, одна строка из data1\n",
        "\n",
        "#Вычисляем количество комнат\n",
        "#maxRoomCount - максимальное число комнат в квартире\n",
        "def getRoomsCount(d, maxRoomCount):\n",
        "  roomsCountStr = d[0] #Получаем строку с числом комнат\n",
        "\n",
        "  roomsCount = 0\n",
        "  try:\n",
        "    roomsCount = int(roomsCountStr) #Пробуем превратить строку в число\n",
        "    if (roomsCount > maxRoomCount): \n",
        "      roomsCount = maxRoomCount #Если число комнат больше максимального, то присваиваем максимальное\n",
        "  except: #Если не получается превратить строку в число\n",
        "    if (roomsCountStr == roomsCountStr): #Проверяем строку на nan (сравнение с самим собой)\n",
        "      if (\"Ст\" in roomsCountStr): #Еcть строка = \"Ст\", значит это Студия\n",
        "        roomsCount = maxRoomCount + 1\n",
        "\n",
        "  return roomsCount\n",
        "\n",
        "#Превращаем число комнат в категорию\n",
        "def getRoomsCountCategory(d, maxRoomCount):\n",
        "  roomsCount = getRoomsCount(d, maxRoomCount) #Получаем число комнат\n",
        "  roomsCount = utils.to_categorical(roomsCount, maxRoomCount+2) #Превращаем в категорию\n",
        "  #maxRoomCount+2 потому что 0 зарезервирован на неопознаное число комнат, а maxRoomCount+1 на \"Студию\"\n",
        "  return roomsCount\n",
        "\n",
        "#Получаем индекс станции метро\n",
        "#allMetroNames - все уникальные названия метро в базе\n",
        "def getMetro(d, allMetroNames):\n",
        "  metroStr = d[1] #Получаем строку метро\n",
        "  metro = 0\n",
        "  \n",
        "  if (metroStr in allMetroNames): #Если находим метро во всех названиях\n",
        "    metro = allMetroNames.index(metroStr)+1 #Присваиваем индекс\n",
        "    #+1 так как 0 зарезервирован на неопознанное метро\n",
        "    \n",
        "  return metro\n",
        "\n",
        "#Получаем тип метро\n",
        "#0 - внутри кольца\n",
        "#1 - кольцо\n",
        "#2 - 1-3 станции от конца\n",
        "#3 - 4-8 станций от кольца\n",
        "#4 - больше 8 станций от кольца\n",
        "def getMetroType(d):\n",
        "  metroTypeStr = d[1] #Получаем строку метро\n",
        "  metroTypeClasses = 5 #Число классов метро\n",
        "  metroType = metroTypeClasses - 1 #Изначально считаем последний класс\n",
        "  \n",
        "  #Метро внутри кольца\n",
        "  metroNamesInsideCircle = [\"Площадь Революции\", \"Арбатская\", \"Смоленская\", \"Красные Ворота\", \"Чистые пруды\", \"Лубянка\", \"Охотный Ряд\", \"Библиотека имени Ленина\", \"Кропоткинская\", \"Сухаревская\", \"Тургеневская\", \"Китай-город\", \"Третьяковская\", \"Трубная\", \"Сретенский бульвар\", \"Цветной бульвар\", \"Чеховская\", \"Боровицкая\", \"Полянка\", \"Маяковская\", \"Тверская\", \"Театральная\", \"Новокузнецкая\", \"Пушкинская\", \"Кузнецкий Мост\", \"Китай-город\", \"Александровский сад\"]\n",
        "  #Метро на кольце\n",
        "  metroNamesCircle = [\"Киевская\", \"Парк Культуры\", \"Октябрьская\", \"Добрынинская\", \"Павелецкая\", \"Таганская\", \"Курская\", \"Комсомольская\", \"Проспект Мира\", \"Новослободская\", \"Белорусская\", \"Краснопресненская\"]\n",
        "  #Метро 1-3 станции от кольца\n",
        "  metroNames13FromCircle = [\"Бауманская\", \"Электрозаводская\", \"Семёновская\", \"Площадь Ильича\", \"Авиамоторная\", \"Шоссе Энтузиастов\", \"Римская\", \"Крестьянская Застава\", \"Дубровка\", \"Пролетарская\", \"Волгоградский проспект\", \"Текстильщики\", \"Автозаводская\", \"Технопарк\", \"Коломенская\", \"Тульская\", \"Нагатинская\", \"Нагорная\", \"Шаболовская\", \"Ленинский проспект\", \"Академическая\", \"Фрунзенская\", \"Спортивная\", \"Воробьёвы горы\", \"Студенческая\", \"Кутузовская\", \"Фили\", \"Парк Победы\", \"Выставочная\", \"Международная\", \"Улица 1905 года\", \"Беговая\", \"Полежаевская\", \"Динамо\", \"Аэропорт\", \"Сокол\", \"Деловой центр\", \"Шелепиха\", \"Хорошёвская\", \"ЦСКА\", \"Петровский парк\", \"Савёловская\", \"Дмитровская\", \"Тимирязевская\", \"Достоевская\", \"Марьина Роща\", \"Бутырская\", \"Фонвизинская\", \"Рижская\", \"Алексеевская\", \"ВДНХ\", \"Красносельская\", \"Сокольники\", \"Преображенская площадь\"]\n",
        "  #Метро 4-8 станций от кольа\n",
        "  metroNames48FromCircle = [\"Партизанская\", \"Измайловская\", \"Первомайская\", \"Щёлковская\", \"Новокосино\", \"Новогиреево\", \"Перово\", \"Кузьминки\", \"Рязанский проспект\", \"Выхино\", \"Лермонтовский проспект\", \"Жулебино\", \"Партизанская\", \"Измайловская\", \"Первомайская\", \"Щёлковская\", \"Новокосино\", \"Новогиреево\", \"Перово\", \"Кузьминки\", \"Рязанский проспект\", \"Выхино\", \"Лермонтовский проспект\", \"Жулебино\", \"Улица Дмитриевского\", \"Кожуховская\", \"Печатники\", \"Волжская\", \"Люблино\", \"Братиславская\", \"Коломенская\", \"Каширская\", \"Кантемировская\", \"Царицыно\", \"Орехово\", \"Севастопольская\", \"Чертановская\", \"Южная\", \"Пражская\", \"Варшавская\", \"Профсоюзная\", \"Новые Черёмушки\", \"Калужская\", \"Беляево\", \"Коньково\", \"Университет\", \"Багратионовская\", \"Филёвский парк\", \"Пионерская\", \"Кунцевская\", \"Молодёжная\", \"Октябрьское Поле\", \"Щукинская\", \"Спартак\", \"Тушинская\", \"Сходненская\", \"Войковская\", \"Водный стадион\", \"Речной вокзал\", \"Беломорская\", \"Ховрино\", \"Петровско-Разумовская\", \"Владыкино\", \"Отрадное\", \"Бибирево\", \"Алтуфьево\", \"Фонвизинская\", \"Окружная\", \"Верхние Лихоборы\", \"Селигерская\", \"ВДНХ\", \"Ботанический сад\", \"Свиблово\", \"Бабушкинская\", \"Медведково\", \"Преображенская площадь\", \"Черкизовская\", \"Бульвар Рокоссовского\"]\n",
        "  \n",
        "  #Проверяем, в какую категорию попадает наша станция\n",
        "  if (metroTypeStr in metroNamesInsideCircle):\n",
        "    metroType = 0\n",
        "  if (metroTypeStr in metroNamesCircle):\n",
        "    metroType = 1\n",
        "  if (metroTypeStr in metroNames13FromCircle):\n",
        "    metroType = 2\n",
        "  if (metroTypeStr in metroNames48FromCircle):\n",
        "    metroType = 3\n",
        "  \n",
        "  #Превращаем результат в категорию\n",
        "  metroType = utils.to_categorical(metroType, metroTypeClasses)\n",
        "  return metroType\n",
        "\n",
        "\n",
        "#Вычисляем растояние до метро\n",
        "def getMetroDistance(d):\n",
        "  metroDistanceStr = d[2] #Получаем строку\n",
        "  \n",
        "  metroDistance = 0 #Расстояние до метро\n",
        "  metroDistanceType = 0 #Тип расстояния - пешком или на транспорте\n",
        "  \n",
        "  #ЕСли строка не равна nan  \n",
        "  if (metroDistanceStr == metroDistanceStr):\n",
        "    if (len(metroDistanceStr) > 0):\n",
        "      #Определяем тип расстояния\n",
        "      if (metroDistanceStr[-1] == \"п\"):\n",
        "        metroDistanceType = 1 #Пешком\n",
        "      elif (metroDistanceStr[-1] == \"т\"):\n",
        "        metroDistanceType = 2 #На транспорте\n",
        "\n",
        "      #Выбрасываем последний символ, чтобы осталось только число\n",
        "      metroDistanceStr = metroDistanceStr[:-1]\n",
        "      try:\n",
        "        #Разделяем дистанции на категории\n",
        "        metroDistance = int(metroDistanceStr)\n",
        "        if (metroDistance < 3):\n",
        "          metroDistance = 1\n",
        "        elif (metroDistance < 6):\n",
        "          metroDistance = 2\n",
        "        elif (metroDistance < 10):\n",
        "          metroDistance = 3\n",
        "        elif (metroDistance < 15):\n",
        "          metroDistance = 4\n",
        "        elif (metroDistance < 20):\n",
        "          metroDistance = 5\n",
        "        else:\n",
        "          metroDistance = 6\n",
        "      except: #Если в строке не число, то категория 0\n",
        "        metroDistance = 0\n",
        "  \n",
        "  #Число классов дистанции\n",
        "  metroDistanceClasses = 7\n",
        "  \n",
        "  #У нас 7 категорий дистанции по расстоянию\n",
        "  #И 3 типа дистанции - неопознанный, пешком и транспортом\n",
        "  #Мы создадим вектор длины 3*7 = 21\n",
        "  #Будем преобразовывать индекс расстояния 0-6 в 0-20\n",
        "  #Для типа \"Пешком\" - ничего не меняем\n",
        "  if (metroDistanceType == 2):\n",
        "    metroDistance += metroDistanceClasses #Для типа \"Транспортом\" добавляем 7\n",
        "  if (metroDistanceType == 0):\n",
        "    metroDistance += 2*metroDistanceClasses #Для неопознанного типа добавляем 14\n",
        "    \n",
        "  #Превращаем в категории\n",
        "  metroDistance = utils.to_categorical(metroDistance, 3*metroDistanceClasses)\n",
        "  return metroDistance\n",
        "\n",
        "#Получаем 4 данных\n",
        "#- этаж квартиры\n",
        "#- этажность дома\n",
        "#- индикатор, что последний этаж\n",
        "#- тип дома\n",
        "def getHouseTypeAndFloor(d):\n",
        "  try:\n",
        "    houseStr = d[3] #Получаем строку типа дома и этажей\n",
        "  except:\n",
        "    houseStr = \"\"\n",
        "  \n",
        "  houseType = 0 #Тип дома\n",
        "  floor = 0 #Этаж квартиры\n",
        "  floors = 0 #Этажность дома\n",
        "  isLastFloor = 0 #Индикатор последнего этажа\n",
        "  \n",
        "  #Проверяем строку на nan\n",
        "  if (houseStr == houseStr):\n",
        "    if (len(houseStr) > 1):\n",
        "    \n",
        "      try:\n",
        "        slashIndex = houseStr.index(\"/\") #Ищем разделитель /\n",
        "      except:\n",
        "        print(houseStr)\n",
        "\n",
        "      try:\n",
        "        spaceIndex = houseStr.index(\" \") #Ищем разделитель \" \"\n",
        "      except:\n",
        "        print(houseStr)\n",
        "\n",
        "      #Вытаскиваем строки\n",
        "      floorStr = houseStr[:slashIndex] #Строка этажа\n",
        "      floorsStr = houseStr[slashIndex+1:spaceIndex] #Строка этажнгости дома\n",
        "      houseTypeStr = houseStr[spaceIndex+1:] #Строка типа дома\n",
        "\n",
        "      #Выбираем категорию этажа\n",
        "      try:\n",
        "        floor = int(floorStr) #Превращаем строку в число\n",
        "        floorSave = floor\n",
        "        if (floorSave < 5):\n",
        "          floor = 2\n",
        "        if (floorSave < 10):\n",
        "          floor = 3\n",
        "        if (floorSave < 20):\n",
        "          floor = 4\n",
        "        if (floorSave >= 20):\n",
        "          floor = 5\n",
        "        if (floorSave == 1): #Первый этаж выделяем в отдельную категорию\n",
        "          floor = 1 \n",
        "\n",
        "        if (floor == floors): #Если этаж последний, включаем индикатор последнего этажа\n",
        "          isLastFloor = 1 \n",
        "      except:\n",
        "        floor = 0 #Если строка не парсится в число, то категория этажа = 0 (отдельная)\n",
        "\n",
        "      #Выбираем категорию этажности дома\n",
        "      try:\n",
        "        floors = int(floorsStr) #Превращаем строку в число\n",
        "        floorsSave = floors\n",
        "        if (floorsSave < 5):\n",
        "          floors = 1\n",
        "        if (floorsSave < 10):\n",
        "          floors = 2\n",
        "        if (floorsSave < 20):\n",
        "          floors = 3\n",
        "        if (floorsSave >= 20):\n",
        "          floors = 4\n",
        "      except:\n",
        "        floors = 0 #Если строка не парсится в число, то категория этажности = 0 (отдельная)\n",
        "\n",
        "      #Определяем категорию типа дома\n",
        "      if (len(houseTypeStr) > 0):\n",
        "        if (\"М\" in houseTypeStr): \n",
        "          houseType = 1\n",
        "        if (\"К\" in houseTypeStr): \n",
        "          houseType = 2\n",
        "        if (\"П\" in houseTypeStr): \n",
        "          houseType = 3\n",
        "        if (\"Б\" in houseTypeStr): \n",
        "          houseType = 4\n",
        "        if (\"?\" in houseTypeStr): \n",
        "          houseType = 5\n",
        "        if (\"-\" in houseTypeStr): \n",
        "          houseType = 6\n",
        "    \n",
        "    #Превращаем все категории в one hot encoding\n",
        "    floor = utils.to_categorical(floor, 6)\n",
        "    floors = utils.to_categorical(floors, 5)\n",
        "    houseType = utils.to_categorical(houseType, 7)\n",
        "    \n",
        "    \n",
        "  return floor, floors, isLastFloor, houseType\n",
        "\n",
        "\n",
        "#Вычисляем тип балкона\n",
        "def getBalcony(d):\n",
        "  balconyStr = d[4] #Полуаем строку\n",
        "  #Выписываем все варианты балконов в базе\n",
        "  balconyVariants = ['Л', 'Б', '2Б', '-', '2Б2Л', 'БЛ', '3Б', '2Л', 'Эрк', 'Б2Л', 'ЭркЛ', '3Л', '4Л', '*Л', '*Б']\n",
        "  #Проверяем на nan\n",
        "  if (balconyStr == balconyStr):\n",
        "    balcony = balconyVariants.index(balconyStr)+1 #Находим индекс строки балкона во всех строках\n",
        "  else:\n",
        "    balcony = 0 #Индекс 0 выделяем на строку nan\n",
        "  \n",
        "  #Превращаем в one hot encoding\n",
        "  balcony = utils.to_categorical(balcony, 16)\n",
        "  \n",
        "  return balcony\n",
        "\n",
        "#Определяем тип санузла\n",
        "def getWC(d):\n",
        "  wcStr = d[5] #Получаем строку\n",
        "  #Выписываем все варианты санузлов в базе\n",
        "  wcVariants = ['2', 'Р', 'С', '-', '2С', '+', '4Р', '2Р', '3С', '4С', '4', '3', '3Р']\n",
        "  #Проверяем на nan\n",
        "  if (wcStr == wcStr):\n",
        "    wc = wcVariants.index(wcStr)+1 #Находим индекс строки санузла во всех строках\n",
        "  else:\n",
        "    wc = 0 #Индекс 0 выделяем на строку nan\n",
        "  \n",
        "  #Превращаем в one hot encoding\n",
        "  wc = utils.to_categorical(wc, 14)\n",
        "  \n",
        "  return wc\n",
        "\n",
        "#Определяем площадь\n",
        "def getArea(d):\n",
        "  areaStr = d[6] #Поулачем строку площади\n",
        "  \n",
        "  if (\"/\" in areaStr):\n",
        "    slashIndex = areaStr.index(\"/\") #Находим разделитель /\n",
        "    try:\n",
        "      area = float(areaStr[:slashIndex]) #Берём число до разделителя и превращаем в число\n",
        "    except:\n",
        "      area = 0 #Если не получается, возвращаем 0\n",
        "  else:\n",
        "    area = 0 #Или если нет разделителя, возвращаем 0\n",
        "    \n",
        "  return area\n",
        "\n",
        "#Полуаем цену\n",
        "def getCost(d):\n",
        "  costStr = d[7] #Загружаем строку\n",
        "  \n",
        "  try:\n",
        "    cost = float(costStr) #Пробуем превратить в число\n",
        "  except:\n",
        "    cost = 0 #Если не получается, возвращаем 0\n",
        "  \n",
        "  return cost\n",
        "\n",
        "#Получаем комментарий\n",
        "def getComment(d):\n",
        "  commentStr = d[-1] #Возвращаем данные из последней колонки\n",
        "  \n",
        "  return commentStr\n",
        "\n",
        "#Объединяем все числовые параметры вместе\n",
        "def getAllParameters(d, allMetroNames):\n",
        "  #Загружаем все данные по отдельности\n",
        "  roomsCountType = getRoomsCountCategory(d, 30)\n",
        "  metro = getMetro(d, allMetroNames)\n",
        "  metroType = getMetroType(d)\n",
        "  metroDistance = getMetroDistance(d)\n",
        "  floor, floors, isLastFloor, houseType = getHouseTypeAndFloor(d)\n",
        "  balcony = getBalcony(d)\n",
        "  wc = getWC(d)\n",
        "  area = getArea(d)\n",
        "\n",
        "  #Объединяем в один лист\n",
        "  out = list(roomsCountType)\n",
        "  out.append(metro)\n",
        "  out.extend(metroType)\n",
        "  out.extend(metroDistance)\n",
        "  out.extend(floor)\n",
        "  out.extend(floors)\n",
        "  out.append(isLastFloor)\n",
        "  out.extend(houseType)\n",
        "  out.extend(balcony)\n",
        "  out.extend(wc)\n",
        "  out.append(area)\n",
        "  \n",
        "  return out\n",
        "\n",
        "#Генерируем обучающаюу выборку - xTrain\n",
        "def getXTrain(data):\n",
        "  \n",
        "  #Получаем строку во всеми вариантами метро\n",
        "  allMertroNames = list(df[\"Метро / ЖД станции\"].unique())\n",
        "  \n",
        "  #Всевращаем все строки в data1 в векторы параметров и записываем в xTrain\n",
        "  xTrain = [getAllParameters(d, allMertroNames) for d in data]\n",
        "  xTrain = np.array(xTrain)\n",
        "  \n",
        "  return xTrain\n",
        "\n",
        "#Генерируем обучающую выборку - yTrain\n",
        "def getYTrain(data):\n",
        "  \n",
        "  #Зашружаем лист всех цен квартир по всем строкам data1\n",
        "  costList = [getCost(d) for d in data] \n",
        "  yTrain = np.array(costList)\n",
        "  \n",
        "  return yTrain"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zn7GMVQylEJD"
      },
      "source": [
        "##Формируем обучающую выборку\n",
        "\n",
        "*Разбор данного раздела:* https://youtu.be/PCMkFDuxvCI?t=6452\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tI264SxulKm8"
      },
      "source": [
        "oneRoomMask = [getRoomsCount(d, 30) == 1 for d in data] #Делаем маску однокомнатных квартир, принцип (getRoomsCount(d, 30) == 1)\n",
        "data1 = data[oneRoomMask] #В data1 оставляем только однокомнатные квартиры\n",
        "print(data.shape)\n",
        "print(data1.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PoVLJhXXlLNL"
      },
      "source": [
        "xTrain = getXTrain(data1)\n",
        "yTrain = getYTrain(data1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fs9C_HDFa_n9"
      },
      "source": [
        "print(xTrain.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s29aVC-mlTXi"
      },
      "source": [
        "##Парсинг текста\n",
        "\n",
        "*Разбор данного раздела:* https://youtu.be/PCMkFDuxvCI?t=6502\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVE4x6YelVgR"
      },
      "source": [
        "###########################\n",
        "# Очистка текста и превращение в набор слов\n",
        "##########################\n",
        "def text2Words(text):\n",
        "\n",
        "  #Удаляем лишние символы\n",
        "  text = text.replace(\".\", \"\")# удаляем лишние символы\n",
        "  text = text.replace(\"—\", \"\")\n",
        "  text = text.replace(\",\", \"\")\n",
        "  text = text.replace(\"!\", \"\")\n",
        "  text = text.replace(\"?\", \"\")\n",
        "  text = text.replace(\"…\", \"\")\n",
        "  text = text.lower() #Переводим в нижний регистр\n",
        "  \n",
        "  \n",
        "  words = [] #Тут будут все слов\n",
        "  currWord = \"\" #Тут будет накапливаться текущее слово, между двумя пробелами\n",
        "  \n",
        "  #идём по всем символам\n",
        "  for symbol in text:\n",
        "    \n",
        "    if (symbol != \"\\ufeff\"): #Игнорируем системынй символ в начале строки\n",
        "      if (symbol != \" \"): #Если символ не пробел\n",
        "        currWord += symbol #То добавляем вимвол в текущее слово\n",
        "      else: #Если символ пробел\n",
        "        if (currWord != \"\"): \n",
        "          words.append(currWord) #Добавляем тккущее слово в список слов\n",
        "          currWord = \"\" #И обнуляем текущее слово\n",
        "\n",
        "  #Добавляем финальное слово, если оно не пустое\n",
        "  #Если не сделать, то потеряем финальное слово, потому что текст чаще всего заканчивается на не пробел\n",
        "  if (currWord != \"\"):\n",
        "        words.append(currWord)\n",
        "  \n",
        "  return words\n",
        "\n",
        "\n",
        "###########################\n",
        "# Создание словаря - все слова, упорядоченные по частоте появления\n",
        "##########################\n",
        "def createVocabulary(allWords):\n",
        "  \n",
        "  #Создаём словарь, в котором будут слова и количество их поялвений во всём текста\n",
        "  #Ключи - все наши слова\n",
        "  #Количество появлений пока везде 0\n",
        "  wCount = dict.fromkeys(allWords, 0)\n",
        "\n",
        "  #Проходим по всем словам\n",
        "  for word in allWords:\n",
        "    wCount[word] += 1 #И увеличиаем количество появлений текущего слова на 1\n",
        "\n",
        "  #Выцепляем лист из словаря\n",
        "  wordsList = list(wCount.items())\n",
        "  #И сортируем по частоте появления\n",
        "  wordsList.sort(key = lambda i:i[1], reverse=1)\n",
        "  #key = lambda i:i[1] - говорит, что сортировать надо по частоте появления\n",
        "  #В i[0] у нас слово, в i[1] - частота появления\n",
        "  #reverse=1 говорить сортироваться по убыванию\n",
        "\n",
        "  sortedWords = [] #Тут будет лист всех отсортированных слов\n",
        "\n",
        "  #Проходим по всем словам в отсортированном списке\n",
        "  for word in wordsList:\n",
        "    sortedWords.append(word[0]) #Докидываем слово в лист отсортированных слов\n",
        "\n",
        "  #Это словарь слово - индекс\n",
        "  #Изначально заполнен всеми словами\n",
        "  #У всех индекс 0\n",
        "  wordIndexes = dict.fromkeys(allWords, 0)\n",
        "  #Проходим по всем словам\n",
        "  for word in wordIndexes.keys():\n",
        "    wordIndexes[word] = sortedWords.index(word)+1 #Ставим индекс = индекс слова в отсортированном листе слов + 1\n",
        "    #+1 потому, что индекс 0 резервируем под неопознанные слова\n",
        "\n",
        "  return wordIndexes\n",
        "\n",
        "\n",
        "###########################\n",
        "# Преобразования листа слов в лист индексов\n",
        "##########################\n",
        "def words2Indexes(words, vocabulary, maxWordsCount):\n",
        "  wordsIndexes = []\n",
        "  \n",
        "  #Идём по всем словая\n",
        "  for word in words:\n",
        "    \n",
        "    wordIndex = 0 #Тут будет индекс слова, изначально 0 - слово неопознано\n",
        "    wordInVocabulary = word in vocabulary #Проверяем, есть ли слово в словаре\n",
        "    \n",
        "    #Если слово есть в словаре\n",
        "    if (wordInVocabulary):\n",
        "      index = vocabulary[word] #Индекс = индексу слова в словаре\n",
        "      if (index < maxWordsCount): #Если индекс ниже maxWordsCount - черты отсечения слов\n",
        "        wordIndex = index #То записываем индекс\n",
        "      #Иначе останется значение 0\n",
        "        \n",
        "    wordsIndexes.append(wordIndex)\n",
        "    \n",
        "  return wordsIndexes\n",
        "\n",
        "\n",
        "###########################\n",
        "# Преобразование одного короткого вектора в вектор из 0 и 1\n",
        "# По принципу words bag\n",
        "##########################\n",
        "def changeXTo01(trainVector, wordsCount):\n",
        "  #Создаём вектор длины wordsCount с нулями\n",
        "  out = np.zeros(wordsCount)\n",
        "  \n",
        "  #Идём по всем индексам в строке\n",
        "  for x in trainVector:\n",
        "    out[x] = 1 #В позицию нужного индекса ставим 1\n",
        "    \n",
        "  return out\n",
        "\n",
        "\n",
        "###########################\n",
        "# Преобразование выборки (обучающей или проверочной) к виду 0 и 1\n",
        "# По принципу words bag\n",
        "##########################\n",
        "def changeSetTo01(trainSet, wordsCount):\n",
        "  out = []\n",
        "  \n",
        "  #Проходим по всем векторам в наборе\n",
        "  for x in trainSet:\n",
        "    out.append(changeXTo01(x, wordsCount)) #Добавляем в итоговый набор текущий вектор, преобразованный в bag of words\n",
        "    \n",
        "  return np.array(out)\n",
        "\n",
        "\n",
        "###########################\n",
        "# Формируем обучающую выборку из примечаний к квартирам\n",
        "# Пока в виде слов\n",
        "##########################\n",
        "def getXTrainComments(data):\n",
        "  xTrainComments = [] #Тут будет обучающся выборка\n",
        "  allTextComments = \"\" #Тут будуте все тексты вместе для словаря\n",
        "  \n",
        "  #Идём по всем строкам квартир в базе\n",
        "  for d in data:\n",
        "    currText = getComment(d) #Вытаскиваем примечание к квартире\n",
        "    try: \n",
        "      if (currText == currText): #Проверяем на nan\n",
        "        allTextComments += currText + \" \" #Добавляем текст в общий текст для словаря\n",
        "    except:\n",
        "      currText = \"Нет комментария\" #Если не получается, то делаем стандартный текст \"Нет комментария\"\n",
        "    xTrainComments.append(currText) #Добавляем примечание новой строкой в обучающую выборку\n",
        "  \n",
        "  xTrainComments = np.array(xTrainComments)\n",
        "  \n",
        "  return (xTrainComments, allTextComments)\n",
        "\n",
        "###########################\n",
        "# Формируем обучающую выборку из примечаний к квартирам\n",
        "# Теперь в виде индексов\n",
        "##########################\n",
        "def changeSetToIndexes(xTrainComments, vocabulary, maxWordsCount):\n",
        "  xTrainCommentsIndexes = [] #Тут будет итоговый xTrain примечаний в виде индексов\n",
        "  \n",
        "  #Идём по всем текстам\n",
        "  for text in xTrainComments:\n",
        "    currWords = text2Words(text) #Разбиваем текст на слова\n",
        "    currIndexes = words2Indexes(currWords, vocabulary, maxWordsCount) #Превращаем в лист индексов\n",
        "    currIndexes = np.array(currIndexes)\n",
        "    xTrainCommentsIndexes.append(currIndexes) #Добавляем в xTrain\n",
        "  \n",
        "  xTrainCommentsIndexes = np.array(xTrainCommentsIndexes)\n",
        "  xTrainCommentsIndexes = changeSetTo01(xTrainCommentsIndexes, maxWordsCount) #Превращаем в формат bag of words\n",
        "  return xTrainCommentsIndexes\n",
        "\n",
        "###########################\n",
        "# Формируем обучающую выборку из примечаний к квартирам\n",
        "# Теперь в виде индексов\n",
        "# И с приведением к стандартной длине всех векторов - cropLen\n",
        "##########################\n",
        "def changeSetToIndexesCrop(xTrainComments, vocabulary, maxWordsCount, cropLen):\n",
        "  xTrainCommentsIndexes = [] #Тут будет итоговый xTrain примечаний в виде индексов\n",
        "  \n",
        "  #Идём по всем текстам\n",
        "  for text in xTrainComments:\n",
        "    currWords = text2Words(text) #Разбиваем текст на слова\n",
        "    currIndexes = words2Indexes(currWords, vocabulary, maxWordsCount) #Превращаем в лист индексов\n",
        "    currIndexes = np.array(currIndexes)\n",
        "    xTrainCommentsIndexes.append(currIndexes) #Добавляем в xTrain\n",
        "  \n",
        "  xTrainCommentsIndexes = np.array(xTrainCommentsIndexes)\n",
        "  xTrainCommentsIndexes = pad_sequences(xTrainCommentsIndexes, maxlen=cropLen) #Приводим все вектора к стандартной длине\n",
        "  return xTrainCommentsIndexes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6Qo4UKqldyH"
      },
      "source": [
        "##Собираем xTrain по текстам\n",
        "\n",
        "*Разбор данного раздела:* https://youtu.be/PCMkFDuxvCI?t=6565\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7lXRfXMlkJW"
      },
      "source": [
        "xTrainC, allTextComments = getXTrainComments(data1) #Создаём обучающую выборку по текстам и большо текст для словаря\n",
        "allWords = text2Words(allTextComments) #Собираем полный текст в слова\n",
        "allWords = allWords[::10] #Берём 10% слов (иначе словарь слишком долго формируется)\n",
        "vocabulary = createVocabulary(allWords) #Создаём словарь\n",
        "xTrainC01 = changeSetToIndexes(xTrainC, vocabulary, 2000) #Преобразеум xTrain в bag of words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93o_A_qTlrki"
      },
      "source": [
        "#Выводим раземры обучающей выборки\n",
        "#Чтобы проверить, что мы всё правильно собрали\n",
        "print(xTrain.shape)\n",
        "print(xTrainC01.shape)\n",
        "print(yTrain.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pgWTHEqlvdF"
      },
      "source": [
        "##Нормировка данных\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjRDPgdMlzg8"
      },
      "source": [
        "#Нормируем размер квартиры в xTrain\n",
        "xScaler = StandardScaler() #Создаём нормировщик нормальным распределением\n",
        "xScaler.fit(xTrain[:,-1].reshape(-1, 1)) #Обучаем его на площадях квартир (последня колонка в xTrain)\n",
        "xTrainScaled = xTrain.copy()\n",
        "xTrainScaled[:,-1] = xScaler.transform(xTrain[:,-1].reshape(-1, 1)).flatten() #Нормируем данные нормировщиком\n",
        "\n",
        "#Выводим размер и два примера для сравнения\n",
        "#Не нормированных данных и нормированных\n",
        "print(xTrainScaled.shape)\n",
        "print(xTrain[0])\n",
        "print(xTrainScaled[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3v3KXrjLl0Qr"
      },
      "source": [
        "#Нормируем выход сети - цену квартиры\n",
        "yScaler = StandardScaler() #Делаемнормальный нормировщик\n",
        "yScaler.fit(yTrain.reshape(-1, 1)) #Обучаем на ценах квартир\n",
        "yTrainScaled = yScaler.transform(yTrain.reshape(-1, 1)) #Нормируем цены квартир\n",
        "\n",
        "#Выводим размер и два примера для сравнения\n",
        "#Не нормированных данных и нормированных\n",
        "print(yTrainScaled.shape)\n",
        "print(yTrain[0])\n",
        "print(yTrainScaled[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZELuSkwUbuGu"
      },
      "source": [
        "print(min(yTrain), max(yTrain))\n",
        "print(min(yTrainScaled), max(yTrainScaled))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZAhclqgl_XP"
      },
      "source": [
        "Формируем проверочную выборку"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQtsSlVFl8qK"
      },
      "source": [
        "splitVal = 0.2 #Процент, который выделяем в проверочную выборку\n",
        "valMask = np.random.sample(xTrainScaled.shape[0]) < splitVal #Создаём маску True-False для создания проверочной выборки"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pn-DuwhittDJ"
      },
      "source": [
        "np.save('/content/drive/My Drive/Colab Notebooks/19 Регрессия/xTrainScaled',xTrainScaled)\n",
        "np.save('/content/drive/My Drive/Colab Notebooks/19 Регрессия/xTrainC01',xTrainC01)\n",
        "np.save('/content/drive/My Drive/Colab Notebooks/19 Регрессия/yTrainScaled',yTrainScaled)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gj-E-3cyb22q"
      },
      "source": [
        "xTrainScaled = np.load('/content/drive/My Drive/Colab Notebooks/19 Регрессия/xTrainScaled.npy')\n",
        "xTrainC01 = np.load('/content/drive/My Drive/Colab Notebooks/19 Регрессия/xTrainC01.npy')\n",
        "yTrainScaled = np.load('/content/drive/My Drive/Colab Notebooks/19 Регрессия/yTrainScaled.npy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMjVNXbkmFv8"
      },
      "source": [
        "##Нейронка\n",
        "\n",
        "*Разбор данного раздела:* https://youtu.be/PCMkFDuxvCI?t=6728"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ay_kbIYeQsc6"
      },
      "source": [
        "xTrainScaled.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q29hms-MQynI"
      },
      "source": [
        "xTrainC01.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7t_LyQfRhoY"
      },
      "source": [
        "trainableParams = [] #Лист тренируемых параметров\n",
        "\n",
        "trainableParams.append(tf.Variable(tf.random.normal([109, 10], stddev=0.03), name='inp1')) \n",
        "trainableParams.append(tf.Variable(tf.random.normal([2000, 250], stddev=0.03), name='inp2')) \n",
        "\n",
        "trainableParams.append(tf.Variable(tf.random.normal([260, 100], stddev=0.03), name='first_hidden')) \n",
        "trainableParams.append(tf.Variable(tf.random.normal([100, 10], stddev=0.03), name='second_hidden')) \n",
        "trainableParams.append(tf.Variable(tf.random.normal([10, 1], stddev=0.03), name='out')) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1B-u6ZiNSibG"
      },
      "source": [
        "def loss(pred , target):\n",
        "    return tf.losses.mse(target , pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hcabr1fAS5Bz"
      },
      "source": [
        "#Полносвязный слой с несколькими функциями активации https://www.youtube.com/watch?v=ljMOTWrszxQ&t=1291s\n",
        "def base(x1, x2, params):\n",
        "    inp1 = params[0]\n",
        "    inp2 = params[1]\n",
        "\n",
        "    first_hidden = params[2]\n",
        "    second_hidden = params[3]\n",
        "    out = params[4]\n",
        "\n",
        "    lay_1 = tf.nn.relu(x1@inp1)\n",
        "    lay_2 = tf.nn.relu(x2@inp2)\n",
        "\n",
        "    lay_conc = tf.concat((lay_1, lay_2), axis=1)\n",
        "\n",
        "    dens_1 = tf.nn.relu(lay_conc@first_hidden)\n",
        "    dens_2 = tf.nn.relu(dens_1@second_hidden)\n",
        "    y = dens_2@out\n",
        "\n",
        "    return y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRSkG9bqg119"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hu4LQOGlg2WF"
      },
      "source": [
        "#Модель  https://www.youtube.com/watch?v=ljMOTWrszxQ&t=1541s\n",
        "def model(x1, x2):\n",
        "    y = base(x1, x2, trainableParams)\n",
        "    return y \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXfcRNqmg2WK"
      },
      "source": [
        "xTrainScaled[~valMask][[0]].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJ6FYK7yg2WM"
      },
      "source": [
        "#Посмотрим, какой выход у модели, если на вход подать случайную картинку \n",
        "model(xTrainScaled[~valMask][[0]], xTrainC01[~valMask][[0]]) #Смотрим вывод сети для первого изображения из MNIST https://www.youtube.com/watch?v=ljMOTWrszxQ&t=1559s"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIH8PHeAg2WN"
      },
      "source": [
        "yTrainScaled[~valMask][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ditTJc2Kg2WN"
      },
      "source": [
        "#Функция для информативного вывода обучения https://www.youtube.com/watch?v=ljMOTWrszxQ&t=1665s\n",
        "#На вход получает:\n",
        "# current - номер текущего батча\n",
        "# amount - число всех батчей \n",
        "# params - словарь дополнительных параметров для вывода \n",
        "\n",
        "def print_log(current, amount, params):\n",
        "  #Мы будем выводить прогрессбар, подобный прогрессбару при обучении нейронных сетей на Keras\n",
        "  #Формат прогрессбара 23/120 [=====>------------------------]\n",
        "  \n",
        "  bar_len = 30 #Длина бара \n",
        "  percent = int(current * bar_len / amount) #Процент выполненной работы\n",
        "  progressbar = ''\n",
        "\n",
        "  for i in range(bar_len): #Проходим по всем элементам прогрессбара и добавляем символы в соответсвии с прогресом \n",
        "    if(i < percent):\n",
        "      progressbar += '='\n",
        "    elif(i == percent):\n",
        "      progressbar += '>'\n",
        "    else:\n",
        "      progressbar += '-'\n",
        "\n",
        "  #Добавялем в финальное сообщение символ переноса коретки консоли на начальную строку, добавляпем информацию о номере батча\n",
        "  #количестве всех батчей, прогрессбар\n",
        "  #Символ переноса коретки \\r добавляется для того, чтобы каждый новый батч перезаписывать вывод. Таким образом вывод не будет засоряться повторяющейся информацией\n",
        "  message = \"\\r\" + str(current) + '/' + str(amount) + ' [' + progressbar + ']  ' \n",
        "  #Добавляем дополнительные параметры в вывод\n",
        "  for key in params:\n",
        "    message += key + str(params[key]) + '. '\n",
        "  \n",
        "  print(message, end='')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLqzy17Rg2WO"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001) #Задаем оптимизатор https://www.youtube.com/watch?v=ljMOTWrszxQ&t=1860s\n",
        "m = tf.keras.metrics.MeanAbsoluteError() #Задаем метрику\n",
        "\n",
        "def train(model, inputs, outputs): #Функция тренировки сети \n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "      current_loss = tf.reduce_mean(loss(model(inputs[0], inputs[1]), outputs)) #Считаем ошибку\n",
        "\n",
        "      # Градиентный спуск. Инициализируем через learning rate\n",
        "      # Функция реализует градиентный спуск и обратное распространение ошибки.\n",
        "      grads = tape.gradient(current_loss , trainableParams)\n",
        "      #Применение градиентного спуска\n",
        "      optimizer.apply_gradients(zip(grads , trainableParams))\n",
        "      #Подсчет точности сети \n",
        "      m.update_state(outputs, model(inputs[0], inputs[1]))\n",
        "    return current_loss, m.result().numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFHI2KGXg2WO"
      },
      "source": [
        "!rm -R /tmp/mylogs/eager #Очищаем папку с записанной информацией обучения (если код запускается не в первый раз)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xta_aWWAsCD7"
      },
      "source": [
        "batchSize = 128\n",
        "epochs = 50"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWaCOK_dg2WP"
      },
      "source": [
        "path = '/tmp/mylogs/eager/' #Указываем путь для сохранения даных для Tensorboard https://www.youtube.com/watch?v=ljMOTWrszxQ&t=2237s\n",
        "\n",
        "\n",
        "current_time = datetime.now().strftime(\"%Y%m%d-%H%M%S\") #Получаем информацию о текущем времени для добавления данной информации к имени  \n",
        "loss_log_dir = path + current_time + '/data' #Создаем папку с именем из текущего времени\n",
        "loss_summary_writer = tf.summary.create_file_writer(loss_log_dir) #Создаем средство записи файла резюме для данного каталога\n",
        "x_len = len(xTrainScaled[~valMask])\n",
        "amount_bathces = int(x_len / batchSize) #Считаем число батчей для каждой эпохи. Понадобится для вывода\n",
        "\n",
        "with loss_summary_writer.as_default(): #Используя созданное средство для записи файла в резюме \n",
        " \n",
        "  for epoch in range(1, epochs + 1): #Проходим по каждой эпохе\n",
        "    learningEpochStartTime = datetime.now() #Запоминаем время начала эпохи \n",
        "    print('Эпоха', epoch , '/', epochs) #Пишем текущую эпоху и общее число эпох\n",
        "    avg_loss = 0 #Задаем среднюю ошибку\n",
        "\n",
        "    for batch in range(0, x_len, batchSize): #Проходим по x_train с шагом batchSize\n",
        "      current_loss, accuracy = train(model, [xTrainScaled[~valMask][batch: batch + batchSize],\n",
        "                                             xTrainC01[~valMask][batch: batch + batchSize]],\n",
        "                                     yTrainScaled[~valMask][batch: batch + batchSize]) #Тренируем сеть и получаем значение ошибки\n",
        "      avg_loss += current_loss #Считаем среднюю ошибку\n",
        "\n",
        "      #Задаем параметры, которые будем выводить \n",
        "      params = {'Время обучения на эпохе: ': datetime.now() - learningEpochStartTime, #Считаем время обучения на данной эпохе и добавляем в словарь\n",
        "                'loss: ': round(current_loss.numpy(), 4), #Переводим ошибку в Numpy и добавляем в словарь\n",
        "                'accuracy: ': round(accuracy, 4)} #Добавляем точность в словарь\n",
        "      if(batch >= x_len - batchSize): #На последнем батче\n",
        "        params['loss: '] = round((avg_loss / amount_bathces).numpy(), 4) #Выводим среднюю ошибку на всей эпохе\n",
        "\n",
        "      current_batch = int(batch / batchSize) + 1 #Считаем номер текущего батча\n",
        "      print_log(current_batch, amount_bathces, params) #Выводим всю нужную информацию \n",
        "    \n",
        "    tf.summary.scalar(\"avg_loss\", avg_loss, step=epoch) #Сохраняем данные для Tensorboard\n",
        "    tf.summary.scalar(\"accuracy\", accuracy, step=epoch) #Сохраняем данные для Tensorboard\n",
        "    loss_summary_writer.flush() #Очищаем буфер вывода \n",
        "    print() #Вручную переносим каретку на следующую строку, чтобы не стирать финальные значения сети на эпохе"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCwvRZXq3NRN"
      },
      "source": [
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnilzRQ13NRN"
      },
      "source": [
        "%tensorboard --logdir '/tmp/mylogs/eager/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yetgNYxh6e1T"
      },
      "source": [
        "# проверим на обучающей выборке\n",
        "for _ in range(5):\n",
        "  i = random.randint(0,len(xTrainScaled[~valMask]))\n",
        "  outp = model(xTrainScaled[~valMask][[i]], xTrainC01[~valMask][[i]])\n",
        "  outp = yScaler.inverse_transform(outp)[0][0]\n",
        "  y_tr = yTrainScaled[~valMask][[i]]\n",
        "  y_tr = yScaler.inverse_transform(y_tr)[0][0]\n",
        "  print('предсказание= ', outp, 'факт= ', y_tr, abs(outp/y_tr*100-100),'%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "StTGh1UO6e1V"
      },
      "source": [
        "# проверим на тестовой выборке\n",
        "for _ in range(5):\n",
        "  i = random.randint(0,len(xTrainScaled[valMask]))\n",
        "  outp = model(xTrainScaled[valMask][[i]], xTrainC01[valMask][[i]])\n",
        "  outp = yScaler.inverse_transform(outp)[0][0]\n",
        "  y_tr = yTrainScaled[valMask][[i]]\n",
        "  y_tr = yScaler.inverse_transform(y_tr)[0][0]\n",
        "  print('предсказание= ', outp, 'факт= ', y_tr, abs(outp/y_tr*100-100),'%')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}